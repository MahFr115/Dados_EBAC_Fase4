{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee35cf2d-15f5-4b39-b26a-66a7b5f39b90",
   "metadata": {},
   "source": [
    "# EBAC \n",
    "## Módulo 24: Combinação de modelos II\n",
    "### Tarefa 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6d1c3-b00d-432e-90d0-7b2f97178fd9",
   "metadata": {},
   "source": [
    "1. Cite 5 diferenças entre o AdaBooste o GBM.\n",
    "2. Acesse o link [Scikit-learn–GBM](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM.\n",
    "3. Cite 5 Hyperparametros importantes no GBM.\n",
    "4. (Opcional) Utilize o GridSearchpara encontrar os melhores hyperparametros para o conjunto de dados do exemplo.\n",
    "5. Acessando o artigo do [Jerome Friedman](https://jerryfriedman.su.domains/ftp/stobst.pdf) (Stochastic) e pensando no nome dado ao StochasticGBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf3bba03-7992-4e50-a7a1-423ebbe63dbc",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th> </th>\n",
    "        <th>GBM</th>\n",
    "        <th>AdaBoost</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Imagem</th>\n",
    "        <td><img src =  https://www.deepchecks.com/wp-content/uploads/2025/02/img-gbm-algorithm-and-llms.jpg width=\"400\"></td>\n",
    "        <td><img src = https://miro.medium.com/v2/da:true/resize:fit:1200/0*sC4kUKp9Wtmi920u width=\"500\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Modelo Aditivo</th>\n",
    "        <td>Gradiente em relação aos resíduos.</td>\n",
    "        <td>Ajuste de pesos.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Profundidade das Árvores</th>\n",
    "        <td>Árvores complexas e inteiras, profundidade, geralmente, de 8 à 32 .</td>\n",
    "        <td>Parte da árveore, com baixa profundidade.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Classificadores</th>\n",
    "        <td>Os classificadores são pesados igualmente, e a capacidade preditiva é restrita com a taxa de aprendizado.</td>\n",
    "        <td>Cada classificador tem pesos diferentes atribuidos à previsão final, baseado em desempenho.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Variável Estudada</th>\n",
    "        <td>Captura a variação nos dados.</td>\n",
    "        <td>Captura a Variância máxima.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Aprendizado</th>\n",
    "        <td>Gradiente descendente para minimizar a função de perda. </td>\n",
    "        <td>Média ponderada das árvores fracas, sendo o resultado final uma soma ponderada das previsões.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Outliers</th>\n",
    "        <td>Modelo mais flexível, com diferentes funções de perda.</td>\n",
    "        <td>Mais sensível a ruidos, pois se concentra em instências classificadas incorretamentes.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Ajuste</th>\n",
    "        <td>Maior controle sobre o modelo, permitindo ajustes finos.</td>\n",
    "        <td>Requer menos ajustes.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db034c1b-3237-4dc7-9801-ec7625abfe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo: Classificação do GBM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Exemplo: Classificação do GBM\")\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train_c, X_test_c = X[:2000], X[2000:]\n",
    "y_train_c, y_test_c = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train_c, y_train_c)\n",
    "clf.score(X_test_c, y_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f20f5d-5dca-4244-b928-83c929fed8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo: Regressão do GBM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Exemplo: Regressão do GBM\")\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train_r, X_test_r = X[:200], X[200:]\n",
    "y_train_r, y_test_r = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train_r, y_train_r)\n",
    "mean_squared_error(y_test_r, est.predict(X_test_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e66e83-5571-417a-920a-6459208e9d61",
   "metadata": {},
   "source": [
    "Hyperparâmetros do GBM:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Hyperparâmetros</th>\n",
    "        <th>Variável</th>\n",
    "        <th>Tipo</th>\n",
    "        <th>Explicação</th>\n",
    "        <th>Valores Frequêntes para Classificação</th>\n",
    "        <th>Valores Frequêntes para Regressão</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Perdas</th>\n",
    "        <td>loss</td>\n",
    "        <td>object</td>\n",
    "        <td>Função otimizada que se refere a binomios e desvios.</td>\n",
    "        <td>- default: log_loss<br>- comuns: exponencial</td>\n",
    "        <td>- default: squared_error<br>- comuns: huber, quantile</td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Número de Estimativas</th>\n",
    "        <td>n_estimators</td>\n",
    "        <td>int</td>\n",
    "        <td>O número máximo de etapas em que cada boosting é determinado.</td>\n",
    "        <td>- default: 100<br>- comuns: [100, 200, 300, 500]</td>\n",
    "        <td>- default: 100<br>- comuns: [200, 300, 500, 1000]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Taxa de Aprendizado</th>\n",
    "        <td>learning_rate</td>\n",
    "        <td>float</td>\n",
    "        <td>Taxa de aprendizado que pondera a contibuição de cada estimador.</td>\n",
    "        <td>- default: 0.1<br>- comuns: [0.01, 0.05, 0.1]</td>\n",
    "        <td>- default: 0.1<br>- comuns: [0.01, 0.05, 0.1]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Flutuação de subamostra</th>\n",
    "        <td>subsample</td>\n",
    "        <td>int</td>\n",
    "        <td>Fração de amostras a utilizar para ajustar a base individual.</td>\n",
    "        <td>- default: 1.0<br>- comuns: [0.6, 0.8, 1.0]</td>\n",
    "        <td>- default: 1.0<br>- comuns: [0.7, 0.8, 1.0]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Função</th>\n",
    "        <td>criterion</td>\n",
    "        <td>object</td>\n",
    "        <td>Função para medir a qualidade de uma divisão</td>\n",
    "        <td>- default: friedman_mse<br>- comuns: squared_error</td>\n",
    "        <td>- default: friedman_mse<br>- comuns: squared_error</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Profundidade Máxima</th>\n",
    "        <td>max_depth</td>\n",
    "        <td>int</td>\n",
    "        <td>Profundidade máxima da árvores, quantidade de níveis de divisão do desenvolvimento da árvore.</td>\n",
    "        <td>- default: 3<br>-comuns: [3, 4, 5, 6]</td>\n",
    "        <td>- default: 3<br>-comuns: [3, 5, 7, 10]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Qtd. de Amostras para Dividir um Nó</th>\n",
    "        <td>min_samples_split</td>\n",
    "        <td>int</td>\n",
    "        <td>Número mínimo de amostragens de dvivisão entre os desenvolvimentos dos nós.</td>\n",
    "        <td>- default: 2<br>- comuns: [2, 4, 5, 10]</td>\n",
    "        <td>- default: 2<br>- comuns: [2, 5, 10]</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760780dd-93e0-4c7d-9699-133f68bf9a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch no exemplo de Classificação\n",
      "Fitting 5 folds for each of 2304 candidates, totalling 11520 fits\n",
      "\n",
      "Melhores parâmetros: {'criterion': 'squared_error', 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 500, 'subsample': 0.6}\n",
      "Melhor score (validação cruzada): 0.9285\n",
      "CPU times: total: 28.5 s\n",
      "Wall time: 37min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"GridSearch no exemplo de Classificação\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cla = GradientBoostingClassifier()\n",
    "\n",
    "# Hiperparâmetros para testar\n",
    "parameters_cla = {\n",
    "    'loss': ['log_loss', 'exponential'],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'criterion':['squared_error', 'friedman_mse'],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 4, 5, 10]\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "clf = GridSearchCV(cla, parameters_cla, cv=5, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Resultados \n",
    "print(f\"\\nMelhores parâmetros: {clf.best_params_}\")\n",
    "print(f\"Melhor score (validação cruzada): {clf.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951a233-45a5-4d42-af22-f1e4789b624c",
   "metadata": {},
   "source": [
    "Nota-se com esse estudo que os valores que alcançam melhor resulatado 'criterion': 'squared_error', 'min_samples_split': 10, 'n_estimators': 500 e 'subsample': 0.6 não são os valores default da função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d5bddd-19a2-4d25-bf50-1b4e6401ba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch no exemplo de Regressão\n",
      "Fitting 5 folds for each of 3240 candidates, totalling 16200 fits\n",
      "\n",
      "Melhores parâmetros: {'criterion': 'squared_error', 'learning_rate': 0.05, 'loss': 'huber', 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "Melhor score (validação cruzada): 0.8373\n",
      "CPU times: total: 37.3 s\n",
      "Wall time: 32min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"GridSearch no exemplo de Regressão\")\n",
    "\n",
    "reg = GradientBoostingRegressor()\n",
    "\n",
    "# Hiperparâmetros para testar\n",
    "parameters_reg = {\n",
    "    'loss': ['squared_error', 'huber', 'quantile'],\n",
    "    'n_estimators': [100, 200, 300, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'criterion':['squared_error', 'friedman_mse'],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "clf = GridSearchCV(reg, parameters_reg, cv=5, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_r, y_train_r)\n",
    "\n",
    "# Resultados \n",
    "print(f\"\\nMelhores parâmetros: {clf.best_params_}\")\n",
    "print(f\"Melhor score (validação cruzada): {clf.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a3829-5781-4440-bd45-61ad600fb348",
   "metadata": {},
   "source": [
    "Nota-se com esse estudo que os valores que alcançam melhor resulatado learning_rate': 0.05, 'loss': 'huber', 'min_samples_split': 10, 'n_estimators': 1000, 'subsample': 0.7 não são os valores default da função."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cdc01-8e94-4a47-a8af-26656794c08a",
   "metadata": {},
   "source": [
    "#### Diferença entre AdaBoots e GBM segundo o artigo\n",
    "A introdução de aleatoriedade no processo de treinamento, mais especificamente na seleção dos dados usados para treinar cada árvore.<br>\n",
    "- Diferença-chave:<br>\n",
    "  - AdaBoots:\n",
    "     - Cada nova árvore é treinada usando todo o conjunto de dados (ou seja, todos os exemplos de treinamento).<br>\n",
    "     - Pode levar ao overfitting, especialmente em datasets ruidosos ou pequenos.\n",
    "  - GBM:\n",
    "     - A cada iteração (ou seja, a cada nova árvore do modelo), é usada apenas uma amostra aleatória dos dados de treino (por exemplo, 50% ou 80% dos dados, sem reposição).<br>\n",
    "     - Isso é semelhante ao conceito de bagging, mas aplicado dentro do boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
